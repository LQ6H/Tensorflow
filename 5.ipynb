{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow--全连接神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.计算步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.激活函数\n",
    "激活函数是神经网络的重要组成部分，为了保证神经网络的灵活性及其计算的复杂度，激活函数一般不会太复杂\n",
    "#### 1.sigmoid激活函数\n",
    "Tensorflow通过函数tf.nn.sigmoid(x,name=None)实现sigmoid激活函数，使用示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.]\n",
      " [ -2.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 输入层\n",
    "x=tf.placeholder(tf.float32,[2,None])\n",
    "\n",
    "# 第1层的权重矩阵\n",
    "w1=tf.constant(\n",
    "    [\n",
    "        [1,4,7],\n",
    "        [2,6,8]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 第1层偏置\n",
    "b1=tf.constant(\n",
    "    [\n",
    "        [-4],\n",
    "        [2],\n",
    "        [1]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 计算第1层的线性组合\n",
    "l1=tf.matmul(w1,x,True)+b1\n",
    "\n",
    "# 激活2*x\n",
    "sigma1=2*l1\n",
    "\n",
    "# 第2层的权重矩阵\n",
    "w2=tf.constant(\n",
    "    [\n",
    "        [2,3],\n",
    "        [1,-2],\n",
    "        [-1,1]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 第2层的偏置\n",
    "b2=tf.constant(\n",
    "    [\n",
    "        [5],\n",
    "        [-3]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 计算第2层的线性组合\n",
    "l2=tf.matmul(w2,sigma1,True)+b2\n",
    "\n",
    "# 激活2*x\n",
    "sigma12=2*l2\n",
    "\n",
    "# 创建会话\n",
    "session=tf.Session()\n",
    "\n",
    "# 令x=[[3],[5]]\n",
    "print(session.run(sigma12,{x:np.array([[3],[5]],np.float32)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  54.  170.]\n",
      " [  94.  330.]\n",
      " [ 134.  490.]\n",
      " [ 174.  650.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 输入层：每一个输入按行存储\n",
    "x=tf.placeholder(tf.float32,(None,2))\n",
    "\n",
    "# 第1层的权重矩阵\n",
    "w1=tf.constant(\n",
    "    [\n",
    "        [1,4,7],\n",
    "        [2,6,8]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 第1层的偏置\n",
    "b1=tf.constant(\n",
    "    [1,4,7],tf.float32\n",
    ")\n",
    "# b1=tf.constant([[1,4,7]],tf.float32) 也可以这样写\n",
    "# 计算第1层的线性组合\n",
    "l1=tf.matmul(x,w1)+b1\n",
    "\n",
    "\n",
    "# 激活2*x\n",
    "sigma1=2*l1\n",
    "\n",
    "# 第2层的权重矩阵\n",
    "w2=tf.constant(\n",
    "    [\n",
    "        [2,3],\n",
    "        [1,-2],\n",
    "        [-1,1]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 第2层偏置\n",
    "b2=tf.constant(\n",
    "    [5,-3],tf.float32\n",
    ")\n",
    "\n",
    "# 计算第2层的线性组合\n",
    "l2=tf.matmul(sigma1,w2)+b2\n",
    "\n",
    "sigma2=2*l2\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(sigma2,{x:np.array([\n",
    "    [10,11],\n",
    "    [20,21],\n",
    "    [30,31],\n",
    "    [40,41]\n",
    "],np.float32)}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.激活函数\n",
    "激活函数是神经网络的重要组成部分，为了保证神经网络的灵活性及其计算的复杂度，激活函数一般不会太复杂\n",
    "#### 1.sigmoid激活函数\n",
    "Tensorflow通过函数tf.nn.sigmoid(x,name=None)实现sigmoid激活函数，使用示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7310586   0.95257413]\n",
      " [ 0.88079703  0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 二维张量\n",
    "t=tf.constant([[1,3],[2,0]],tf.float32)\n",
    "\n",
    "# 激活sigmod激活\n",
    "result=tf.nn.sigmoid(t)\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76159418  0.99505472]\n",
      " [ 0.96402758  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 二维张量\n",
    "t=tf.constant([[1,3],[2,0]],tf.float32)\n",
    "\n",
    "# 激活sigmod激活\n",
    "result=tf.nn.tanh(t)\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.ReLU激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.]], dtype=float32), array([[ 2.,  3.,  4.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 变量\n",
    "x=tf.Variable(tf.constant([[2,1,3]],tf.float32))\n",
    "\n",
    "w=tf.constant([[2],[3],[4]],tf.float32)\n",
    "\n",
    "g=tf.matmul(x,w)\n",
    "\n",
    "f=tf.nn.relu(g)\n",
    "\n",
    "# 计算f在(2,1,3)处的导数\n",
    "gradient=tf.gradients(f,[g,x])\n",
    "\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "print(session.run(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "relu_layer() got an unexpected keyword argument 'aplha'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b088313b7a7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maplha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# 牛顿梯度下降法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mopti\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: relu_layer() got an unexpected keyword argument 'aplha'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 变量\n",
    "x=tf.Variable(tf.constant([[2,1,3]],tf.float32))\n",
    "w=tf.constant([[2],[3],[4]],tf.float32)\n",
    "\n",
    "g=tf.matmul(x,w)\n",
    "\n",
    "f=tf.nn.relu_layer(g,aplha=0.2)\n",
    "# 牛顿梯度下降法\n",
    "opti=tf.train.GradientDescentOptimizer(0.5).minimize(f)\n",
    "\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(3):\n",
    "    session.run(opti)\n",
    "    \n",
    "    print(\"第%d次迭代的值\"%(i+1))\n",
    "    print(session.run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### elu激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  2. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t=tf.constant([-2,0,1],tf.float32)\n",
    "\n",
    "r=tf.nn.crelu(t)\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
