{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow--卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络与全连接神经网络类似，可以理解为一种变换，这种变换一般由卷积，池化，加法，激活函数等一系列操作组合而成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.浅层卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "输入的三维张量首先与3个2行2列2深度的卷积核进行步长为1的same卷积，输出结果的尺寸是3行3列3深度\n",
    "对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 输入张量\n",
    "input_tensor=tf.constant(\n",
    "    [\n",
    "        # 第1个高为3，宽为3，深度为2的三维张量\n",
    "        [\n",
    "            [[2,5],[3,3],[8,2]],\n",
    "            [[6,1],[1,2],[5,4]],\n",
    "            [[7,9],[2,8],[1,3]]\n",
    "        ]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 3个高为2，宽为2，深度为2的卷积核\n",
    "kernel=tf.constant(\n",
    "    [\n",
    "        [[[-1,1,0],[1,-1,-1],[0,0,-1],[0,0,0]]],\n",
    "        [[[0,0,0],[0,0,-1],[1,-1,1],[-1,1,0]]]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 卷积\n",
    "conv2d=tf.nn.conv2d(input_tensor,kernel,(1,1,1,1),'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "将得到的卷积结果在每一个深度上加一个常数(即加偏置)\n",
    "对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 偏置\n",
    "bias=tf.constant([1,2,3],tf.float32)\n",
    "conv2d_add_bias=tf.add(conv2d,bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "用ReLU激活函数处理得到的结果\n",
    "对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "active=tf.nn.relu(conv2d_add_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "将激活后的结果，经过2x2的步长为1的valid最大值池化操作\n",
    "对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pool操作\n",
    "active_maxPool=tf.nn.max_pool(active,(1,2,2,1),(1,1,1,1),'VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "然后将结果进行拉伸操作\n",
    "对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 拉伸\n",
    "shape=active_maxPool.get_shape()\n",
    "num=shape[1].value*shape[2].value+shape[3].value\n",
    "flatten=tf.reshape(active_maxPool,[-1,num])\n",
    "# 打印结果\n",
    "session=tf.Session()\n",
    "print(session.run(flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "完整代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 13. 12.  2.  8.  5.  7. 13. 12.  7.  3.  5.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 输入张量\n",
    "input_tensor=tf.constant(\n",
    "    [\n",
    "        # 第1个高为3，宽为3，深度为2的三维张量\n",
    "        [\n",
    "            [[2,5],[3,3],[8,2]],\n",
    "            [[6,1],[1,2],[5,4]],\n",
    "            [[7,9],[2,8],[1,3]]\n",
    "        ]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 3个高为2，宽为2，深度为2的卷积核\n",
    "kernel=tf.constant(\n",
    "    [\n",
    "        [[[-1,1,0],[1,-1,-1]],[[0,0,-1],[0,0,0]]],\n",
    "        [[[0,0,0],[0,0,1]],[[1,-1,1],[-1,1,0]]]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 卷积\n",
    "conv2d=tf.nn.conv2d(input_tensor,kernel,(1,1,1,1),'SAME')\n",
    "\n",
    "# 偏置\n",
    "bias=tf.constant([1,2,3],tf.float32)\n",
    "conv2d_add_bias=tf.add(conv2d,bias)\n",
    "\n",
    "# 激活函数\n",
    "active=tf.nn.relu(conv2d_add_bias)\n",
    "\n",
    "# pool操作\n",
    "active_maxPool=tf.nn.max_pool(active,(1,2,2,1),(1,1,1,1),'VALID')\n",
    "\n",
    "# 拉伸\n",
    "shape=active_maxPool.get_shape()\n",
    "num=shape[1].value*shape[2].value*shape[3].value\n",
    "flatten=tf.reshape(active_maxPool,[-1,num])\n",
    "# 打印结果\n",
    "session=tf.Session()\n",
    "print(session.run(flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，输入的三维张量经过一系列变换后，转换为了长度为12的向量，上述变换称为卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们介绍如何实现输入3个不同的三维张量，即这3个张量分别经过该网络的结构，具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 13. 12.  2.  8.  5.  7. 13. 12.  7.  3.  5.]\n",
      " [ 5.  9.  8.  5.  3.  7.  6.  9.  8.  6.  3.  7.]\n",
      " [ 3.  4.  5.  2.  4.  5.  1.  5.  5.  2.  5.  5.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 输入张量\n",
    "input_tensor=tf.placeholder(tf.float32,[None,3,3,2])\n",
    "\n",
    "# 3个高为2，宽为2，深度为2的卷积核\n",
    "kernel=tf.constant(\n",
    "    [\n",
    "        [[[-1,1,0],[1,-1,-1]],[[0,0,-1],[0,0,0]]],\n",
    "        [[[0,0,0],[0,0,1]],[[1,-1,1],[-1,1,0]]]\n",
    "    ]\n",
    "    ,tf.float32\n",
    ")\n",
    "\n",
    "# 卷积\n",
    "conv2d=tf.nn.conv2d(input_tensor,kernel,(1,1,1,1),'SAME')\n",
    "\n",
    "# 偏置\n",
    "bias=tf.constant([1,2,3],tf.float32)\n",
    "conv2d_add_bias=tf.add(conv2d,bias)\n",
    "\n",
    "# 激活函数\n",
    "active=tf.nn.relu(conv2d_add_bias)\n",
    "\n",
    "# pool操作\n",
    "active_maxPool=tf.nn.max_pool(active,(1,2,2,1),(1,1,1,1),'VALID')\n",
    "\n",
    "# 拉伸\n",
    "shape=active_maxPool.get_shape()\n",
    "num=shape[1].value*shape[2].value*shape[3].value\n",
    "flatten=tf.reshape(active_maxPool,[-1,num])\n",
    "# flatten=tf.contrib.layers.flatten(active_maxPool)\n",
    "\n",
    "session=tf.Session()\n",
    "print(session.run(flatten,feed_dict={\n",
    "    input_tensor:np.array([\n",
    "        # 第1个3行3列2深度的三维张量\n",
    "        [\n",
    "            [[2,5],[3,3],[8,2]],\n",
    "            [[6,1],[1,2],[5,4]],\n",
    "            [[7,9],[2,8],[1,3]]\n",
    "        ],\n",
    "         # 第2个3行3列2深度的三维张量\n",
    "        [\n",
    "            [[1,2],[3,6],[1,2]],\n",
    "            [[3,1],[1,2],[2,1]],\n",
    "            [[4,5],[2,7],[1,2]]\n",
    "        ],\n",
    "        [\n",
    "         # 第3个3行3列2深度的三维张量\n",
    "            [[2,3],[3,2],[1,2]],\n",
    "            [[4,1],[3,2],[1,2]],\n",
    "            [[1,0],[4,1],[4,3]]\n",
    "        ]\n",
    "    ],np.float32)\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解了简单的卷积神经网络后，接下类介绍经典的卷积神经网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet是第一个成熟的卷积神经网络，是专门为处理MNIST数字字符集的分类问题而设计的网络结构，该网络结构输入值的尺寸是32行32列1深度的三维张量(灰色图像)，其经过LeNet网络的变换过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第1步：与6个高为5，宽为5，深度为1的卷积核valid卷积，然后将卷积结果的每一深度加偏置，最后将结果经过激活函数处理，得到的结果的尺寸高为28，宽为28，深度为6\n",
    "第1步的对应代码如下，其中卷积核和偏置都由满足高斯分布的随机数生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6个高为5，宽为5，深度为1的卷积核\n",
    "k1=tf.Variable(tf.random_normal([5,5,1,6]),dtype=tf.float32)\n",
    "c1=tf.nn.conv2d(x,k1,[1,1,1,1],'VALID')\n",
    "#长度为6的偏置\n",
    "b1=tf.Variable(tf.random_normal([6]),dtype=tf.float32)\n",
    "# c1与b1求和，并激活函数\n",
    "c1_b1=tf.add(c1,b1)\n",
    "r1=tf.nn.relu(c1_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第2步：进行2x2的步长为2的valid最大值池化，池化后结果的尺寸高为14，宽为14，深度为6\n",
    "第2步对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最大值池化操作，池化掩码的尺寸为高2宽2，步长为2\n",
    "p1=tf.nn.max_pool(r1,[1,2,2,1],[1,2,2,1],'VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3步：先与16个高为5，宽为5，深度为6的卷积核valid卷积，然后将卷积结果在每一深度上加偏置，最后将加上偏置的结果经过激活处理。得到的结果的尺寸高为10，宽为10，深度为16\n",
    "第3步对应的代码如下，其中卷积核和偏置都是由满足高斯分布的随机数生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p1与16个高为5，宽为5，深度为6的卷积核的卷积\n",
    "k2=tf.Variable(tf.random_normal([5,5,6,16]),dtype=tf.float32)\n",
    "c2=tf.nn.conv2d(p1,k2,[1,1,1,1],'VALID')\n",
    "# 长度为16的偏置\n",
    "b2=tf.Variable(tf.random_normal([16]),dtype=tf.float32)\n",
    "# c2与b2求和，并输入激活函数\n",
    "c2_b2=tf.add(c2,b2)\n",
    "r2=tf.nn.relu(c2_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第4步：进行2x2的步长为2的valid最大值池化，池化后结果的尺寸是高为5，宽为5，深度为16\n",
    "第4步对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最大值池化操作，池化掩码的尺寸是高为2，宽为2，步长为2\n",
    "p2=tf.nn.max_pool(c2,[1,2,2,1],[1,2,2,1],'VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第5步：将第4步得到的结果拉伸为1个一维张量，其长度为5x5x16=400，然后将这个向量经过一个全连接神经网络处理，该全连接神经网络有3个隐含层，其中输入层有400个神经元，第1层隐含层有120个神经元，第2层隐含层有84个神经元。因为要处理的数据有10个类别，所以输出层有10个神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过这5步变换，我们将1个高为32，宽为32，深度为1的张量转换成了1个长度为10的向量，其中第5步对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-6-3b39044b0933>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-3b39044b0933>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    h1=tf.add(tf.matmul(flatten_p2，w1),bw1)\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# 拉伸为一维张量，作为一个全连接神经网络的输入\n",
    "flatten_p2=tf.reshape(p2,[-1,5*5*16])\n",
    "# 第1层的权重矩阵和偏置\n",
    "w1=tf.Variable(tf.random_normal([5*5*16,120]))\n",
    "bw1=tf.Variable(tf.random_normal([120]))\n",
    "# 第1层的线性组合与偏置求和，并作为relu激活函数的输入\n",
    "h1=tf.add(tf.matmul(flatten_p2，w1),bw1)\n",
    "sigma1=tf.nn.relu(h1)\n",
    "# 第2层的权重矩阵和偏置\n",
    "w2=tf.Variable(tf.random_normal([120,84]))\n",
    "bw2=tf.Variable(tf.random_normal([84]))\n",
    "# 第2层的线性组合与偏置求和，并作为relu激活函数的输入\n",
    "h2=tf.add(tf.matmul(sigma1,w2),bw2)\n",
    "sigma2=tf.nn.relu(h2)\n",
    "# 第3层线性组合与偏置求和\n",
    "w3=tf.Variable(tf.random_normal([84,10]))\n",
    "bw3=tf.Variable(tf.random_normal([10]))\n",
    "h3=tf.add(tf.matmul(sigma2,w3),bw3)\n",
    "# 将h3作为sigmod激活函数的输入，作为最后输出层的输出\n",
    "out=tf.nn.sigmoid(h3)\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "# 初始化变量\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.local_variables_initializer())\n",
    "\n",
    "print(session.run(out,{x:np.random.normal(0,1,[]2,32,32,1)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'filter' of 'Conv2D' Op has type float32 that does not match type float64 of argument 'input'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_TensorConversionFunction\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    827\u001b[0m           \u001b[1;34m\"Incompatible type conversion requested to type '%s' for variable \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m           \"of type '%s'\" % (dtype.name, v.dtype.name))\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible type conversion requested to type 'float64' for variable of type 'float32_ref'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3ecf2cc80dc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 6个高为5，宽为5，深度为1的卷积核\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mc1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'VALID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#长度为6的偏置\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m   1044\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    544\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 546\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'filter' of 'Conv2D' Op has type float32 that does not match type float64 of argument 'input'."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 6个高为5，宽为5，深度为1的卷积核\n",
    "k1=tf.Variable(tf.random_normal([5,5,1,6]),dtype=tf.float32)\n",
    "x=np.random.normal(0,1,[2,32,32,1])\n",
    "c1=tf.nn.conv2d(x,k1,[1,1,1,1],'VALID')\n",
    "#长度为6的偏置\n",
    "b1=tf.Variable(tf.random_normal([6]),dtype=tf.float32)\n",
    "# c1与b1求和，并激活函数\n",
    "c1_b1=tf.add(c1,b1)\n",
    "r1=tf.nn.relu(c1_b1)\n",
    "\n",
    "# 最大值池化操作，池化掩码的尺寸为高2宽2，步长为2\n",
    "p1=tf.nn.max_pool(r1,[1,2,2,1],[1,2,2,1],'VALID')\n",
    "\n",
    "# p1与16个高为5，宽为5，深度为6的卷积核的卷积\n",
    "k2=tf.Variable(tf.random_normal([5,5,6,16]),dtype=tf.float32)\n",
    "c2=tf.nn.conv2d(p1,k2,[1,1,1,1],'VALID')\n",
    "# 长度为16的偏置\n",
    "b2=tf.Variable(tf.random_normal([16]),dtype=tf.float32)\n",
    "# c2与b2求和，并输入激活函数\n",
    "c2_b2=tf.add(c2,b2)\n",
    "r2=tf.nn.relu(c2_b2)\n",
    "\n",
    "# 最大值池化操作，池化掩码的尺寸是高为2，宽为2，步长为2\n",
    "p2=tf.nn.max_pool(c2,[1,2,2,1],[1,2,2,1],'VALID')\n",
    "\n",
    "# 拉伸为一维张量，作为一个全连接神经网络的输入\n",
    "flatten_p2=tf.reshape(p2,[-1,5*5*16])\n",
    "# 第1层的权重矩阵和偏置\n",
    "w1=tf.Variable(tf.random_normal([5*5*16,120]))\n",
    "bw1=tf.Variable(tf.random_normal([120]))\n",
    "# 第1层的线性组合与偏置求和，并作为relu激活函数的输入\n",
    "h1=tf.add(tf.matmul(flatten_p2,w1),bw1)\n",
    "sigma1=tf.nn.relu(h1)\n",
    "# 第2层的权重矩阵和偏置\n",
    "w2=tf.Variable(tf.random_normal([120,84]))\n",
    "bw2=tf.Variable(tf.random_normal([84]))\n",
    "# 第2层的线性组合与偏置求和，并作为relu激活函数的输入\n",
    "h2=tf.add(tf.matmul(sigma1,w2),bw2)\n",
    "sigma2=tf.nn.relu(h2)\n",
    "# 第3层线性组合与偏置求和\n",
    "w3=tf.Variable(tf.random_normal([84,10]))\n",
    "bw3=tf.Variable(tf.random_normal([10]))\n",
    "h3=tf.add(tf.matmul(sigma2,w3),bw3)\n",
    "# 将h3作为sigmod激活函数的输入，作为最后输出层的输出\n",
    "out=tf.nn.sigmoid(h3)\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "# 初始化变量\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.local_variables_initializer())\n",
    "\n",
    "print(session.run(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet卷积神经网络主要是针对高为224，宽为224的彩色图像的分类问题设计的卷积神经网络结构。AlexNet比LeNet具备更深的网络结构卷积核的深度页更深，全连接的神经元个数也更多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.AlexNet网络结构详解\n",
    "理解成针对224行224列3深度的三维张量的变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet卷积神经网络处理的是1000个分类的任务，所以最后全连接神经网络的输出层的神经元个数是1000,具体过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 输入\n",
    "x=tf.placeholder(tf.float32,[None,224,224,3])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "# 第1步：与96个11x11x3的卷积核卷积。第2步：加上偏置。第3步：使用激活函数\n",
    "w1=tf.Variable(tf.random_normal([11,11,3,96]),dtype=tf.float32,name='w1')\n",
    "l1=tf.nn.conv2d(x,w1,[1,4,4,1],'SAME')\n",
    "b1=tf.Variable(tf.random_normal([96]),dtype=tf.float32,name='b1')\n",
    "l1=tf.nn.bias_add(l1,b1)\n",
    "l1=tf.nn.relu(l1)\n",
    "\n",
    "# 2x2最大值池化操作，移动步长为2\n",
    "pool_l1=tf.nn.max_pool(l1,[1,2,2,1],[1,2,2,1],'SAME')\n",
    "\n",
    "# 第1步：与256个5x5x96的卷积核卷积。第2步：加上偏置。第3步：使用激活函数\n",
    "w2=tf.Variable(tf.random_normal([5,5,96,256]),dtype=tf.float32,name='w2')\n",
    "l2=tf.nn.conv2d(pool_l1,w2,[1,1,1,1],'SAME')\n",
    "b2=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b2')\n",
    "l2=tf.nn.bias_add(l2,b2)\n",
    "l2=tf.nn.relu(l2)\n",
    "\n",
    "# 2x2最大值池化操作，移动步长为2\n",
    "pool_l2=tf.nn.max_pool(l2,[1,2,2,1],[1,2,2,1],'SAME')\n",
    "\n",
    "# 第1步：与384个3x3x256的卷积核卷积。第2步：加上偏置。第3步：使用激活函数\n",
    "w3=tf.Variable(tf.random_normal([3,3,256,384]),dtype=tf.float32,name='w3')\n",
    "l3=tf.nn.conv2d(pool_l2,w3,[1,1,1,1],'SAME')\n",
    "b3=tf.Variable(tf.random_normal([384]),dtype=tf.float32,name='b3')\n",
    "l3=tf.nn.bias_add(l3,b3)\n",
    "l3=tf.nn.relu(l3)\n",
    "\n",
    "# 第1步：与384个3x3x384的卷积核卷积。第2步：加上偏置。第3步：使用激活函数\n",
    "w4=tf.Variable(tf.random_normal([3,3,384,384]),dtype=tf.float32,name='w4')\n",
    "l4=tf.nn.conv2d(l3,w4,[1,1,1,1],'SAME')\n",
    "b4=tf.Variable(tf.random_normal([384]),dtype=tf.float32,name='b4')\n",
    "l4=tf.nn.bias_add(l4,b4)\n",
    "l4=tf.nn.relu(l4)\n",
    "\n",
    "# 第1步：与256个3x3x384的卷积核卷积。第2步：加上偏置。第3步：使用激活函数\n",
    "w5=tf.Variable(tf.random_normal([3,3,384,256]),dtype=tf.float32,name='w5')\n",
    "l5=tf.nn.conv2d(l4,w5,[1,1,1,1],'SAME')\n",
    "b5=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b5')\n",
    "l5=tf.nn.bias_add(l5,b5)\n",
    "l5=tf.nn.relu(l5)\n",
    "\n",
    "# 2x2最大值池化操作，移动步长为2\n",
    "pool_l5=tf.nn.max_pool(l5,[1,2,2,1],[1,2,2,1],'SAME')\n",
    "\n",
    "# 拉伸，作为全连接神经网络的输入层\n",
    "pool_l5_shape=pool_l5.get_shape()\n",
    "num=pool_l5_shape[1].value*pool_l5_shape[2].value*pool_l5_shape[3].value\n",
    "flatten=tf.reshape(pool_l5,[-1,num])\n",
    "\n",
    "# 第1个隐含层\n",
    "fcW1=tf.Variable(tf.random_normal([num,4096]),dtype=tf.float32,name='fcW1')\n",
    "fc_l1=tf.matmul(flatten,fcW1)\n",
    "fcb1=tf.Variable(tf.random_normal([4096]),dtype=tf.float32,name='fcb1')\n",
    "fc_l1=tf.nn.bias_add(fc_l1,fcb1)\n",
    "fc_l1=tf.nn.relu(fc_l1)\n",
    "fc_l1=tf.nn.dropout(fc_l1,keep_prob)\n",
    "\n",
    "# 第2个隐含层\n",
    "fcW2=tf.Variable(tf.random_normal([4096,4096]),dtype=tf.float32,name='fcW2')\n",
    "fc_l2=tf.matmul(fc_l1,fcW2)\n",
    "fcb2=tf.Variable(tf.random_normal([4096]),dtype=tf.float32,name='fcb2')\n",
    "fc_l2=tf.nn.bias_add(fc_l2,fcb2)\n",
    "fc_l2=tf.nn.relu(fc_l2)\n",
    "fc_l2=tf.nn.dropout(fc_l2,keep_prob)\n",
    "\n",
    "# 输出层\n",
    "fcW3=tf.Variable(tf.random_normal([4096,1000]),dtype=tf.float32,name='fcW3')\n",
    "out=tf.matmul(fc_l2,fcW3)\n",
    "fcb3=tf.Variable(tf.random_normal([1000]),dtype=tf.float32,name='fcb3')\n",
    "out=tf.nn.bias_add(out,fcb3)\n",
    "out=tf.nn.relu(out)\n",
    "\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "result=session.run(out,feed_dict={x:np.ones([2,224,224,3],np.float32),keep_prob:0.5})\n",
    "\n",
    "print(np.shape(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet一共包括5个卷积层和3个全连接层，所以介绍AlexNet时，一般会说该网络有8层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet在算法上有两个非常重要的改进。第1个改进是提出ReLU激活函数。第2个改进是提出dropout，该操作可以有效地防止网络的过拟合，所谓的过拟合就是网络结构在训练集上的准确率很高，但在测试集上的准确率较低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.dropout极其梯度下降\n",
    "Tensorflow中的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout(x,keep_prob,noise_shape=None,seed=None,name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中参数x代表输入张量，参数keep_prob属于区间(0,1]中的一个值，输入张量x中每一个值以概率keep_prob变为原来的1/keep_prob倍，以概率1-keep_prob变为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以2行4列的二维张量为例，假设该张量作为函数dropout的参数x的值，令参数keep_prob=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  4. 12.]\n",
      " [14. 10.  0. 18.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 输入的二维张量\n",
    "t=tf.constant(\n",
    "        [\n",
    "        [1,3,2,6],\n",
    "        [7,5,4,9]\n",
    "        ],tf.float32\n",
    "        )\n",
    "\n",
    "# dropout处理\n",
    "r=tf.nn.dropout(t,0.5)\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次运行的结果可能有所不同。显然，张量中的值要么变为原来的1/0.5倍，要么变为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上示例的基础上，我们来介绍参数noise_shape的作用，因为张量的尺寸为2行4列，如果令参数noise_shape=[2,1]，则代表把每一行看成一个整体，同一行的值要么全变为原来的1/keep_prob倍，要么全变为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  6.  4. 12.]\n",
      " [14. 10.  8. 18.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 输入的二维张量\n",
    "t=tf.constant(\n",
    "        [\n",
    "        [1,3,2,6],\n",
    "        [7,5,4,9]\n",
    "        ],tf.float32\n",
    "        )\n",
    "\n",
    "# dropout处理\n",
    "r=tf.nn.dropout(t,0.5,noise_shape=[2,1])\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，如果参数noise_shape=[1,4]，代表把每一列看成一个整体，同一列的值要么全变为原来的1/keep_prob倍，要么全变为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.  4. 12.]\n",
      " [14.  0.  8. 18.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 输入的二维张量\n",
    "t=tf.constant(\n",
    "        [\n",
    "        [1,3,2,6],\n",
    "        [7,5,4,9]\n",
    "        ],tf.float32\n",
    "        )\n",
    "\n",
    "# dropout处理\n",
    "r=tf.nn.dropout(t,0.5,noise_shape=[1,4])\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout层\n",
    "dropout处理一般用在全连接神经网络的全连接层或者卷积神经网络后面的全连接层\n",
    "\n",
    "仍以[2,3]和[1,4]作为网络的两个输入，通过以下代码分别经过该网络变换后的输入值，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "隐含层的值：\n",
      "[[ 8. 18. 28.]\n",
      " [ 9. 19. 29.]]\n",
      "dropout层的值：\n",
      "[[ 0.  0. 56.]\n",
      " [ 0. 38. 58.]]\n",
      "输出层的值：\n",
      "[[336.  56.]\n",
      " [614. 134.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 占位符\n",
    "x=tf.placeholder(tf.float32,[None,2])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "# 输入层到隐含层的权重矩阵\n",
    "w1=tf.constant([\n",
    "        [1,3,5],\n",
    "        [2,4,6]\n",
    "        ],tf.float32)\n",
    "\n",
    "# 隐含层的值\n",
    "h1=tf.matmul(x,w1)\n",
    "\n",
    "# dropout层\n",
    "h1_dropout=tf.nn.dropout(h1,keep_prob)\n",
    "\n",
    "# dropout层到输出层的权重矩阵\n",
    "w2=tf.constant(\n",
    "        [\n",
    "        [8,3],\n",
    "        [7,2],\n",
    "        [6,1]\n",
    "        ],tf.float32\n",
    "        )\n",
    "\n",
    "# 输出层的值\n",
    "o=tf.matmul(h1_dropout,w2)\n",
    "x_input=np.array([[2,3],[1,4]],np.float32)\n",
    "\n",
    "session=tf.Session()\n",
    "h1_arr,h1_dropout_arr,o_arr=s=session.run([h1,h1_dropout,o],feed_dict={x:x_input,keep_prob:0.5})\n",
    "\n",
    "print('隐含层的值：')\n",
    "print(h1_arr)\n",
    "print(\"dropout层的值：\")\n",
    "print(h1_dropout_arr)\n",
    "print('输出层的值：')\n",
    "print(o_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout梯度下降\n",
    "我们通过简单的网络结构理解有dropout层的全连接神经网络的梯度反向传播，其中dropout对应的概率为0.5，输入层，隐含层，输出层都只有一个神经元，激活函数为x\n",
    "\n",
    "接下来我们初始化w11=10，w11=6，针对该函数进行标准梯度下降，其中学习率为0.01，具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----第\"1\"次迭代----\n",
      "dropout层的值：\n",
      "[[0.]]\n",
      "网络当前的权重：\n",
      "[array([[10.]], dtype=float32), array([[6.]], dtype=float32)]\n",
      "----第\"2\"次迭代----\n",
      "dropout层的值：\n",
      "[[0.]]\n",
      "网络当前的权重：\n",
      "[array([[10.]], dtype=float32), array([[6.]], dtype=float32)]\n",
      "----第\"3\"次迭代----\n",
      "dropout层的值：\n",
      "[[60.]]\n",
      "网络当前的权重：\n",
      "[array([[9.64]], dtype=float32), array([[5.4]], dtype=float32)]\n",
      "----第\"4\"次迭代----\n",
      "dropout层的值：\n",
      "[[0.]]\n",
      "网络当前的权重：\n",
      "[array([[9.64]], dtype=float32), array([[5.4]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 占位符，已知数据\n",
    "x=tf.placeholder(tf.float32,(None,1))\n",
    "keep_pro=tf.placeholder(tf.float32)\n",
    "\n",
    "# 输入层到隐含层的权重矩阵\n",
    "w1=tf.Variable(tf.constant([[10]],tf.float32),dtype=tf.float32)\n",
    "l1=tf.matmul(x,w1)\n",
    "\n",
    "# dropout层\n",
    "l1_dropout=tf.nn.dropout(l1,keep_pro)\n",
    "\n",
    "# 隐含层到输出层的权重矩阵\n",
    "w2=tf.Variable(tf.constant([[6]],tf.float32),dtype=tf.float32)\n",
    "l=tf.matmul(l1_dropout,w2)\n",
    "\n",
    "# 利用网络输出值的构造函数f\n",
    "f=tf.reduce_sum(l)\n",
    "\n",
    "# 梯度下降法\n",
    "opti=tf.train.GradientDescentOptimizer(0.01).minimize(f)\n",
    "#\"���������ֵ\"\n",
    "x_array=np.array([[3]],np.float32)\n",
    "\n",
    "# 4次迭代，打印每一次迭代隐含层的值和网络权重\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(4):\n",
    "        _,l1_dropout_array=session.run([opti,l1_dropout],\n",
    "                            {x:x_array,keep_pro:0.5})\n",
    "        print('----第\"{}\"次迭代----'.format(i+1))\n",
    "        print(\"dropout层的值：\")\n",
    "        print(l1_dropout_array)\n",
    "        print('网络当前的权重：')\n",
    "        print(session.run([w1,w2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGGNet比AlexNet的网络层数多，不再使用尺寸较大的卷积核，如11x11,7x7,5x5，而是只采用尺寸为3x3的卷积核。这一点可以从卷积核的分离性理解，即计算量变小。以输入值的尺寸是224行224列3深度，接下来我们利用Tensorflow实现该网络结构\n",
    "\n",
    "输入层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.placeholder(tf.float32,[None,224,224,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第1层：输入层与64个3行3列3深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer1',reuse=tf.AUTO_REUSE):\n",
    "    # 64个3行3列3深度的卷积核\n",
    "    w1=tf.Variable(tf.random_normal([3,3,3,64]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c1=tf.nn.conv2d(x,w1,[1,1,1,1],'SAME')\n",
    "    # 因为c1的深度为64，所以偏置的长度为64\n",
    "    b1=tf.Variable(tf.random_normal([64]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果于偏置相加\n",
    "    c1=tf.nn.bias_add(c1,b1)\n",
    "    # relu激活函数\n",
    "    c1=tf.nn.relu(c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第2层：将第1层的结果(其尺寸为224行224列64深度)与64个3行3列3深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer2',reuse=tf.AUTO_REUSE):\n",
    "    # 64个3行3列3深度的卷积核\n",
    "    w2=tf.Variable(tf.random_normal([3,3,64,64]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c2=tf.nn.conv2d(c1,w2,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为64，所以偏置的长度为64\n",
    "    b2=tf.Variable(tf.random_normal([64]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c2=tf.nn.bias_add(c2,b2)\n",
    "    # relu激活函数\n",
    "    c2=tf.nn.relu(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过第2层后，输出结果的尺寸仍是224行224列64深度，接着对其进行2x2的步长为2的最大值池化操作，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对c2进行same最大值池化操作，邻域掩码的尺寸为2行2列，步长为2\n",
    "p_c2=tf.nn.max_pool(c2,[1,2,2,1],[1,2,2,1],'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3层：池化操作，结果的尺寸为112行112列64深度，也就是将高和宽的尺寸变小了，为了保证每一层的计算量差距不太大，在宽和高方向上虽然尺寸变小了，想办法在深度方向上增加尺寸即可，将池化结果与128个3行3列64深度的卷积核卷积，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer3',reuse=tf.AUTO_REUSE):\n",
    "    # 128个3行3列64深度的卷积核\n",
    "    w3=tf.Variable(tf.random_normal([3,3,64,128]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c3=tf.nn.conv2d(p_c2,w3,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为128，所以偏置的长度为128\n",
    "    b3=tf.Variable(tf.random_normal([128]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c3=tf.nn.bias_add(c3,b3)\n",
    "    # relu激活函数\n",
    "    c3=tf.nn.relu(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第4层：经过第3层后，输出结果的尺寸仍为112行112列128深度，接着与128个3行3列128深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer4',reuse=tf.AUTO_REUSE):\n",
    "    # 128个3行3列128深度的卷积核\n",
    "    w4=tf.Variable(tf.random_normal([3,3,128,128]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c4=tf.nn.conv2d(c3,w4,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为128，所以偏置的长度为128\n",
    "    b4=tf.Variable(tf.random_normal([128]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c4=tf.nn.bias_add(c4,b4)\n",
    "    # relu激活函数\n",
    "    c4=tf.nn.relu(c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过第4层后，输出结果的尺寸为112列112行128深度，接着对其进行2x2的步长为2的最大值池化操作，对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对c4进行same最大值池化操作，邻域掩码的尺寸为2行2列，步长均为2\n",
    "p_c4=tf.nn.max_pool(c4,[1,2,2,1],[1,2,2,1],'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第5层：池化操作后的结果的尺寸为56行56列128深度，显然这时宽和高的尺寸减小了，为了在深度方向上增加尺寸，接着与256个3行3列128深度卷积核卷积，将卷积结果加偏置，输入ReLU激活函数，对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer5',reuse=tf.AUTO_REUSE):\n",
    "    # 1256个3行3列128深度的卷积核\n",
    "    w5=tf.Variable(tf.random_normal([3,3,128,256]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c5=tf.nn.conv2d(p_c4,w5,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为256，所以偏置的长度为256\n",
    "    b5=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c5=tf.nn.bias_add(c5,b5)\n",
    "    # relu激活函数\n",
    "    c5=tf.nn.relu(c5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第6层：经过第5层后，输出结果的尺寸为56行56列256深度，接着与256个3行3列256深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer6',reuse=tf.AUTO_REUSE):\n",
    "    # 256个3行3列256深度的卷积核\n",
    "    w6=tf.Variable(tf.random_normal([3,3,256,256]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c6=tf.nn.conv2d(c5,w6,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为256，所以偏置的长度为256\n",
    "    b6=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c6=tf.nn.bias_add(c6,b6)\n",
    "    # relu激活函数\n",
    "    c6=tf.nn.relu(c6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第7层：经过第6层后，输出结果的尺寸为56行56列256深度，接着与256个3行3列256深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer7',reuse=tf.AUTO_REUSE):\n",
    "    # 256个3行3列256深度的卷积核\n",
    "    w7=tf.Variable(tf.random_normal([3,3,256,256]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c7=tf.nn.conv2d(c6,w7,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为256，所以偏置的长度为256\n",
    "    b6=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c7=tf.nn.bias_add(c7,b7)\n",
    "    # relu激活函数\n",
    "    c7=tf.nn.relu(c7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过第7层后，输出结果的尺寸为56行56列256深度，接着对其进行2x2的步长为2的最大值池化操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c7=tf.nn.max_pool(c7,[1,2,2,1],[1,2,2,1],'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第8层：池化操作后的结果的尺寸为28行28列256深度，显然这时宽和高的尺寸减小了，为了在深度方向上增加尺寸，接着与512个3行3列256深度卷积核卷积，将卷积结果加偏置，输入ReLU激活函数，对应的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer8',reuse=tf.AUTO_REUSE):\n",
    "    # 512个3行3列256深度的卷积核\n",
    "    w8=tf.Variable(tf.random_normal([3,3,256,512]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c8=tf.nn.conv2d(p_c7,w8,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为512，所以偏置的长度为512\n",
    "    b8=tf.Variable(tf.random_normal([512]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c8=tf.nn.bias_add(c8,b8)\n",
    "    # relu激活函数\n",
    "    c8=tf.nn.relu(c8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第9层：经过第8层后，输出结果的尺寸为28行28列512深度，接着与512个3行3列512深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer9',reuse=tf.AUTO_REUSE):\n",
    "    # 512个3行3列512深度的卷积核\n",
    "    w9=tf.Variable(tf.random_normal([3,3,512,512]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c9=tf.nn.conv2d(c8,w9,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为512，所以偏置的长度为512\n",
    "    b9=tf.Variable(tf.random_normal([512]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c9=tf.nn.bias_add(c9,b9)\n",
    "    # relu激活函数\n",
    "    c9=tf.nn.relu(c9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第10层：经过第9层后，输出结果的尺寸为28行28列512深度，接着与512个3行3列512深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer10',reuse=tf.AUTO_REUSE):\n",
    "    # 512个3行3列512深度的卷积核\n",
    "    w10=tf.Variable(tf.random_normal([3,3,512,512]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c10=tf.nn.conv2d(c9,w10,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为512，所以偏置的长度为512\n",
    "    b10=tf.Variable(tf.random_normal([512]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c10=tf.nn.bias_add(c10,b10)\n",
    "    # relu激活函数\n",
    "    c10=tf.nn.relu(c10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过第10层后，输出结果的尺寸为28行28列512深度，接着对其进行2x2的步长为2的最大值池化操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c10=tf.nn.max_pool(c10,[1,2,2,1],[1,2,2,1],'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第11层：经过第10层后，输出结果的尺寸为14行14列512深度，接着与512个3行3列512深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer11',reuse=tf.AUTO_REUSE):\n",
    "    # 512个3行3列512深度的卷积核\n",
    "    w11=tf.Variable(tf.random_normal([3,3,512,512]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c11=tf.nn.conv2d(p_c10,w11,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为512，所以偏置的长度为512\n",
    "    b11=tf.Variable(tf.random_normal([512]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c11=tf.nn.bias_add(c11,b11)\n",
    "    # relu激活函数\n",
    "    c11=tf.nn.relu(c11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第12层：经过第11层后，输出结果的尺寸为14行14列512深度，接着与512个3行3列512深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer12',reuse=tf.AUTO_REUSE):\n",
    "    # 512个3行3列512深度的卷积核\n",
    "    w12=tf.Variable(tf.random_normal([3,3,512,512]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c12=tf.nn.conv2d(c12,w12,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为512，所以偏置的长度为512\n",
    "    b12=tf.Variable(tf.random_normal([512]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c12=tf.nn.bias_add(c12,b12)\n",
    "    # relu激活函数\n",
    "    c12=tf.nn.relu(c12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第13层：经过第12层后，输出结果的尺寸为14行14列512深度，接着与512个3行3列512深度的卷积核same卷积，将卷积结果加偏置，然后输入ReLU激活函数，对应代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer13',reuse=tf.AUTO_REUSE):\n",
    "    # 512个3行3列512深度的卷积核\n",
    "    w13=tf.Variable(tf.random_normal([3,3,512,512]),dtype=tf.float32,name='w')\n",
    "    # 步长为1的same卷积\n",
    "    c13=tf.nn.conv2d(c12,w13,[1,1,1,1],'SAME')\n",
    "    # 因为c2的深度为512，所以偏置的长度为512\n",
    "    b13=tf.Variable(tf.random_normal([512]),dtype=tf.float32,name='b')\n",
    "    # 卷积结果与偏置相加\n",
    "    c13=tf.nn.bias_add(c13,b13)\n",
    "    # relu激活函数\n",
    "    c13=tf.nn.relu(c13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过第13层后，输出结果的尺寸为14行14列512深度，接着对其进行2x2的步长为2的最大值池化操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c13=tf.nn.max_pool(c13,[1,2,2,1],[1,2,2,1],'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上述池化操作结果进行拉伸操作，其结果可以看作一个全连接神经网络的输入层，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape=p_c13.get_shape()\n",
    "flatten_p_c13=tf.reshape(p_c13,[-1,shape[1]*shape[2]*shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第14层：作为全连接神经网络的第1个隐含层，其神经元的个数为4096，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer14\",reuse=tf.AUTO_REUSE):\n",
    "    # 权重矩阵和偏置\n",
    "    w14=tf.Variable(\n",
    "        tf.random_normal([shape[1].value*shape[2].value*shape[3].value,4096]),dtype=tf.float32,name='w'\n",
    "    )\n",
    "    b14=tf.Variable(tf.random_normal([4096]),dtype=tf.float32,name='b')\n",
    "    # 线性组合\n",
    "    fc14=tf.matmul(flatten_p_c13,w14)\n",
    "    fc14=tf.nn.bias_add(fc14,b14)\n",
    "    # relu线性组合激活\n",
    "    fc14=tf.nn.relu(fc14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第15层：作为全连接神经网络的第2个隐含层，其神经元的个数为4096，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer15\",reuse=tf.AUTO_REUSE):\n",
    "    # 权重矩阵和偏置\n",
    "    w15=tf.Variable(\n",
    "        tf.random_normal([4096,4096]),dtype=tf.float32,name='w'\n",
    "    )\n",
    "    b15=tf.Variable(tf.random_normal([4096]),dtype=tf.float32,name='b')\n",
    "    # 线性组合\n",
    "    fc15=tf.matmul(fc14,w15)\n",
    "    fc15=tf.nn.bias_add(fc15,b15)\n",
    "    # relu线性组合激活\n",
    "    fc15=tf.nn.relu(fc15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第16层：作为全连接神经网络的输出层，其神经元的个数为4096，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer16\",reuse=tf.AUTO_REUSE):\n",
    "    # 权重矩阵和偏置\n",
    "    w16=tf.Variable(\n",
    "        tf.random_normal([4096,1000]),dtype=tf.float32,name='w'\n",
    "    )\n",
    "    b16=tf.Variable(tf.random_normal([1000]),dtype=tf.float32,name='b')\n",
    "    # 线性组合\n",
    "    fc16=tf.matmul(fc15,w16)\n",
    "    fc16=tf.nn.bias_add(fc16,b16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建完以上网络结构后，打印VGGNet后面的全连接神经网络的输入层的神经元个数，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(shape[1].value*shape[2].value*shape[3].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印结果为：25088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为第13层经过池化操作的尺寸为7行7列512深度，该张量拉伸为一个一维向量的长度为：7x7x512=25088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 五.GoogleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleNet是基于网中网(Network In Network)的一种网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.网中网结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过以下示例说明。1个3行3列2深度的张量与7个2行2列2深度的卷积核或者7个3行3列2深度的卷积核same卷积，两种都可以得到结果的尺寸为3行3列7深度。网中网结构通过多个分支的运算(分支的运算可以为卷积也可以为池化)，将分支上的运算结果在深度上连接，得到的结果的尺寸为3行3列7深度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述示例有3个分支，第1个分支：3行3列2深度的张量先与3个1行1列2深度的卷积核same卷积，其结果的尺寸为3行3列3深度。第2个分支：与2个2行2列2深度的卷积核same卷积，其结果的尺寸为3行3列2深度。第3个分支：进行3x3的步长为1的same最大值池化操作，其结果为3行3列2深度，将这三个分支上的结果在深度的方向上连接，最后的结果为3行3列7深度，上述示例的具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3. -6.  8.]\n",
      "   [ 0.  0.  3.]\n",
      "   [ 6. 12. -4.]]\n",
      "\n",
      "  [[ 5. 10. -4.]\n",
      "   [-1. -2.  3.]\n",
      "   [ 1.  2.  3.]]\n",
      "\n",
      "  [[-2. -4. 11.]\n",
      "   [ 5. 10. -8.]\n",
      "   [-4. -8.  7.]]]]\n",
      "[[[[ 16.  43.]\n",
      "   [ 33.  58.]\n",
      "   [  9.  29.]]\n",
      "\n",
      "  [[-20.  65.]\n",
      "   [ 21.  11.]\n",
      "   [ 11.  23.]]\n",
      "\n",
      "  [[ 20.   4.]\n",
      "   [ 11.   0.]\n",
      "   [  0.   7.]]]]\n",
      "[[[[6. 5.]\n",
      "   [8. 5.]\n",
      "   [8. 4.]]\n",
      "\n",
      "  [[7. 9.]\n",
      "   [8. 9.]\n",
      "   [8. 4.]]\n",
      "\n",
      "  [[7. 9.]\n",
      "   [7. 9.]\n",
      "   [5. 4.]]]]\n",
      "[[[[ -3.  -6.   8.  16.  43.   6.   5.]\n",
      "   [  0.   0.   3.  33.  58.   8.   5.]\n",
      "   [  6.  12.  -4.   9.  29.   8.   4.]]\n",
      "\n",
      "  [[  5.  10.  -4. -20.  65.   7.   9.]\n",
      "   [ -1.  -2.   3.  21.  11.   8.   9.]\n",
      "   [  1.   2.   3.  11.  23.   8.   4.]]\n",
      "\n",
      "  [[ -2.  -4.  11.  20.   4.   7.   9.]\n",
      "   [  5.  10.  -8.  11.   0.   7.   9.]\n",
      "   [ -4.  -8.   7.   0.   7.   5.   4.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1个高为3，宽为3，深度为2的输入张量\n",
    "inputTensor=tf.constant(\n",
    "        [\n",
    "        [\n",
    "        [[2,5],[3,3],[8,2]],\n",
    "        [[6,1],[1,2],[5,4]],\n",
    "        [[7,9],[2,-3],[-1,3]]\n",
    "        ]\n",
    "        ],tf.float32\n",
    "        )\n",
    "#\n",
    "session=tf.Session()\n",
    "\n",
    "# 3个高为1，宽为1，深度为2的卷积核\n",
    "filter112_3=tf.constant(\n",
    "        [\n",
    "        [[[1,2,-1],[-1,-2,2]]]\n",
    "        ],tf.float32\n",
    "        )\n",
    "result1=tf.nn.conv2d(inputTensor,filter112_3,[1,1,1,1],'SAME')\n",
    "print(session.run(result1))\n",
    "\n",
    "# 2个高为2，宽为2，深度为2的卷积核\n",
    "filter222_2=tf.constant(\n",
    "        [\n",
    "        [[[3,-1],[1,2]],[[-2,1],[2,3]]],\n",
    "        [[[-1,1],[-3,7]],[[4,2],[5,4]]]\n",
    "        ],tf.float32)\n",
    "result2=tf.nn.conv2d(inputTensor,filter222_2,[1,1,1,1],'SAME')\n",
    "print(session.run(result2))\n",
    "\n",
    "# 最大值池化\n",
    "maxPool_33=tf.nn.max_pool(inputTensor,[1,3,3,1],[1,1,1,1],'SAME')\n",
    "print(session.run(maxPool_33))\n",
    "\n",
    "# 深度方向连接\n",
    "result=tf.concat([result1,result2,maxPool_33],3)\n",
    "print(session.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleNet是基于类似网中网模块设计的网络结构，在GoogleNet中该模块称为Inception Module，多个Inception Module模块的组合即为GoogleNet\n",
    "Inception Module有多种变形，接下来我们介绍通过1x1的卷积核修改的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用1x1的卷积核减少计算成本\n",
    "假设有1个高为30，宽为40，深度为200的三维张量与55个高为5，宽为5，深度为200的卷积核same卷积，假设移动步长均为1，则卷积结果是高为30，宽为40，深度为55的三维张量\n",
    "该卷积过程的乘法计算量大约为5x5x200x30x40x55=330000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们考虑以下卷积过程\n",
    "第1步：高为30，宽为40，深度为200的三为张量，先与20个高为1，宽为1，深度为20的卷积核same卷积，假设步长为1，即在深度方向上降维，得到1个高为30，宽为40，深度为20的三维张量\n",
    "第2步：与55个高为5，宽为5，深度为20的卷积核same卷积，移动步长为1，得到高为30，宽为40，深度为55的卷积结果\n",
    "我们来计算上述卷积过程的总乘法计算量：\n",
    "第1步的乘法计算量：1x1x200x30x40=4800000\n",
    "第2步的乘法计算量：5x5x20x30x40x55=33000000\n",
    "总的乘法计算量为：33000000+4800000=37800000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，同样是得到高为30，宽为40，深度为55的卷积结果，采用第2种方式，即首先在深度方向上降维，比第1中计算方式在计算量上减少了8.7倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对GoogleNet的改进，还需要特别介绍的就是著名的Batch Normalization(BN归一化，简称BN)，它有效地解决了在网络层数很深的情况下，收敛速度很慢的问题，使用BN操作可加大网络梯度下降的学习率，加速网络收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Batch Normalization\n",
    "Tensorflow提供函数batch_normalization(x,mean,variance,offset,scale,variance_epsilon,name=None)实现BN操作的功能，其中参数mean和参数variance的值(即均值和方差)是通过另一个函数moments(x,axes,shitf=None)进行计算的，利用这两个函数实现以上示例的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4096959  -0.28193915  1.3470427   0.34459233]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x=tf.constant([1,10,23,15],tf.float32)\n",
    "\n",
    "# 计算均值和方差\n",
    "mean,variance=tf.nn.moments(x,[0])\n",
    "# BatchNormalize\n",
    "r=tf.nn.batch_normalization(x,mean,variance,0,1,1e-8)\n",
    "session=tf.Session()\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设在训练网络时，每次抽取2组训练数据，有2个2行2列2深度的三维张量，它是某2个样本数据经过某网络的某一卷积层时的结果，然后分别在每一深度上进行BN操作，假设对第1层进行BN操作时的γ=2,β=3；对第2层深度进行BN操作时的γ=5，β=8，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值和方差：\n",
      "[array([ 5. , 14.5], dtype=float32), array([7.5, 9. ], dtype=float32)]\n",
      "BN操作后的结果：\n",
      "[[[[ 0.0788132  3.833332 ]\n",
      "   [ 3.730297  13.833334 ]]\n",
      "\n",
      "  [[ 5.921187   5.5      ]\n",
      "   [ 2.2697034  2.166666 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.8091099 15.5      ]\n",
      "   [ 4.4605937 12.166666 ]]\n",
      "\n",
      "  [[ 1.5394068  8.833334 ]\n",
      "   [ 5.1908903  2.166666 ]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 思维张量\n",
    "t=tf.constant(\n",
    "        [\n",
    "        # 第1个2行2列2深度的三维张量\n",
    "        [\n",
    "        [[1,12],[6,18]],\n",
    "        [[9,13],[4,11]],\n",
    "        ],\n",
    "        # 第2个2行2列2深度的三维张量\n",
    "        [\n",
    "        [[2,19],[7,17]],\n",
    "        [[3,15],[8,11]]\n",
    "        ]\n",
    "        ],tf.float32\n",
    "        )\n",
    "\n",
    "# 计算均值和方差 moments\n",
    "mean,variance=tf.nn.moments(t,[0,1,2])#[0,1,2]\n",
    "# BatchNormalize\n",
    "gamma=tf.Variable(tf.constant([2,5],tf.float32))\n",
    "beta=tf.Variable(tf.constant([3,8],tf.float32))\n",
    "r=tf.nn.batch_normalization(t,mean,variance,beta,gamma,1e-8)\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "print('均值和方差：')\n",
    "print(session.run([mean,variance]))\n",
    "\n",
    "# BatchNormalize的结果\n",
    "print('BN操作后的结果：')\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.指数移动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次的移动平均值：\n",
      "5.0\n",
      "第2次的移动平均值：\n",
      "6.5\n",
      "第3次的移动平均值：\n",
      "9.05\n",
      "第4次的移动平均值：\n",
      "12.335\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 初始化变量，即t=1时的值\n",
    "x=tf.Variable(initial_value=5,dtype=tf.float32,trainable=False,name='v')\n",
    "\n",
    "# 创建计算移动平均的对象\n",
    "exp_moving_avg=tf.train.ExponentialMovingAverage(0.7)\n",
    "update_moving_avg=exp_moving_avg.apply([x])\n",
    "\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "for i in range(4):\n",
    "    # 打印指数移动平均值\n",
    "    session.run(update_moving_avg)\n",
    "    print('第{}次的移动平均值：'.format(i+1))\n",
    "    print(session.run(exp_moving_avg.average(x)))\n",
    "    session.run(x.assign_add(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.带有BN操作的卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中输入的样本数据的尺寸为1行2列3深度，所有的偏置均为0，每次设计从以下代码中解析2个张量，作为该卷积神经网络的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_3' with dtype bool\n\t [[node Placeholder_3 (defined at <ipython-input-6-ef43c9b97e68>:5)  = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_3', defined at:\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ef43c9b97e68>\", line 5, in <module>\n    trainable=tf.placeholder(tf.bool)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6251, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_3' with dtype bool\n\t [[node Placeholder_3 (defined at <ipython-input-6-ef43c9b97e68>:5)  = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_3' with dtype bool\n\t [[{{node Placeholder_3}} = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef43c9b97e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0marrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---第%(num)d 批array---'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'num'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_3' with dtype bool\n\t [[node Placeholder_3 (defined at <ipython-input-6-ef43c9b97e68>:5)  = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_3', defined at:\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ef43c9b97e68>\", line 5, in <module>\n    trainable=tf.placeholder(tf.bool)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6251, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"E:\\Anaconda\\envs\\mytensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_3' with dtype bool\n\t [[node Placeholder_3 (defined at <ipython-input-6-ef43c9b97e68>:5)  = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#\"输入值的占位符\"\n",
    "x=tf.placeholder(tf.float32,[None,1,2,3])\n",
    "#\"设置是否是训练阶段的占位符\"\n",
    "trainable=tf.placeholder(tf.bool)\n",
    "#\"移动平均\"\n",
    "ema=tf.train.ExponentialMovingAverage(0.7)\n",
    "ema_var_list=[]\n",
    "#\"----------第一层-----------\"\n",
    "#\"2个1行1列3深度的卷积核\"\n",
    "k1=tf.Variable(tf.constant([\n",
    "                [[[1,0],[0,1],[0,0]]],\n",
    "                ],tf.float32)\n",
    "        )\n",
    "#\"偏置\"\n",
    "b1=tf.Variable(tf.zeros(2))\n",
    "#\"卷积结果加偏置\"\n",
    "c1=tf.nn.conv2d(x,k1,[1,1,1,1],'SAME')+b1\n",
    "beta1=tf.Variable(tf.zeros(c1.get_shape()[-1].value))\n",
    "gamma1=tf.Variable(tf.ones(c1.get_shape()[-1].value))\n",
    "#\"计算每一深度上的均值和方差\"\n",
    "m1,v1=tf.nn.moments(c1,[0,1,2])\n",
    "ema_var_list+=[m1,v1]\n",
    "#\"为了保存均值和方差的指数移动平均\"\n",
    "m1_ema=tf.Variable(tf.zeros(c1.get_shape()[-1]),trainable=False)\n",
    "v1_ema=tf.Variable(tf.zeros(c1.get_shape()[-1]),trainable=False)\n",
    "#\"BN操作\"\n",
    "c1_BN=tf.cond(trainable,\n",
    "        lambda:tf.nn.batch_normalization(c1,m1,v1,beta1,gamma1,1e-8),\n",
    "        lambda: tf.nn.batch_normalization(c1,m1_ema,\n",
    "                                          v1_ema,beta1,gamma1,1e-8)\n",
    "        )\n",
    "#\"relu激活\"\n",
    "r1=tf.nn.relu(c1_BN)\n",
    "#\"----------第二层-----------\"\n",
    "#\"2个1行1列2深度的卷积核\"\n",
    "k2=tf.Variable(tf.constant(\n",
    "        [\n",
    "        [[[2,0],[0,2]]]\n",
    "        ],tf.float32\n",
    "        ))\n",
    "#\"偏置\"\n",
    "b2=tf.Variable(tf.zeros(2))\n",
    "#\"卷积结果加偏置\"\n",
    "c2=tf.nn.conv2d(r1,k2,[1,1,1,1],'SAME')+b2\n",
    "beta2=tf.Variable(tf.zeros(c2.get_shape()[-1]))\n",
    "gamma2=tf.Variable(tf.ones(c2.get_shape()[-1]))\n",
    "#\"计算每一深度上的均值和方差\"\n",
    "m2,v2=tf.nn.moments(c2,[0,1,2])\n",
    "ema_var_list+=[m2,v2]\n",
    "#\"为了保存均值和方差的指数移动平均\"\n",
    "m2_ema=tf.Variable(tf.zeros(c1.get_shape()[-1]),trainable=False)\n",
    "v2_ema=tf.Variable(tf.zeros(c1.get_shape()[-1]),trainable=False)\n",
    "#\"BN操作\"\n",
    "c2_BN=tf.cond(trainable,\n",
    "        lambda:tf.nn.batch_normalization(c2,m2,v2,beta2,gamma2,1e-8),\n",
    "        lambda:tf.nn.batch_normalization(c2,m2_ema,\n",
    "                                         v2_ema,beta2,gamma2,1e-8)\n",
    "        )\n",
    "#\"relu激活\"\n",
    "r2=tf.nn.relu(c2_BN)\n",
    "update_moving_avg=ema.apply(ema_var_list)\n",
    "#\"创建会话\"\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.local_variables_initializer())\n",
    "coord=tf.train.Coordinator()\n",
    "threads=tf.train.start_queue_runners(sess=session,coord=coord)\n",
    "num=2\n",
    "for i in range(num):\n",
    "    arrs=session.run(r2)\n",
    "    print('---第%(num)d 批array---'%{'num':i+1})\n",
    "    print(arrs)\n",
    "    _,c1_arr=session.run([update_moving_avg,c1],\n",
    "                        feed_dict={x:arrs,trainable:True})\n",
    "    print('---第%(num)d 次迭代后第1个卷积层(卷积结果加偏置)的值---'%{'num':i+1})\n",
    "    print(c1_arr)\n",
    "    #\"将计算的指数移动平均的值赋值给 Variable 对象\"\n",
    "    session.run(m1_ema.assign(ema.average(m1)))\n",
    "    session.run(v1_ema.assign(ema.average(v1)))\n",
    "    session.run(m2_ema.assign(ema.average(m2)))\n",
    "    session.run(v2_ema.assign(ema.average(v2)))\n",
    "    print('---第%(num)d 次迭代后第1个卷积层的均值的移动平均值---'%{'num':i+1})\n",
    "    print(session.run(m1_ema))\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
